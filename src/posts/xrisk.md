---
title: The existential risk of superintelligent AI
description: Why we all might die
published: true
---
## Many AI experts are sounding the alarm

Over half of AI researchers think there’s a larger than 10% risk that a when we build a superintelligent AI, it will take over and destroy our planet ([source](https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/)). When you ask AI safety researchers, this number grows to 30% ([source](https://forum.effectivealtruism.org/posts/8CM9vZ2nnQsWJNsHx/existential-risk-from-ai-survey-results)).

The [letter for pausing AI development](https://futureoflife.org/open-letter/pause-giant-ai-experiments/), launched in april 2023, has been signed over 27.000 times, mostly by AI researchers and tech leaders.

## It's not just a few people

- **Stuart Russel**, writer of the #1 textbook on Artificial Intelligence
- Geoffrey Hinton, the "Godfather of AI" and inventor of neural network, [left Google]https://fortune.com/2023/05/01/godfather-ai-geoffrey-hinton-quit-google-regrets-lifes-work-bad-actors/() to warn people of AI.
- Elon Musk, co-founder of OpenAI, SpaceX and **Tesla**
- Stephen Hawking, theoretical physicist
- Eliezer Yudkowski, founder of MIRI and conceptual father of the AI safety field

## Things are developing really, really fast

We’ve all noticed how insane AI capabilities have become in the past months. In 2020, [the average AI researcher though](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/) that it would take until 2057 before an AI could pass SAT exams. _It took us less than 3 years_.

## "It’s like ‘Don’t Look Up’, but we’re building the asteroid ourselves"

##
