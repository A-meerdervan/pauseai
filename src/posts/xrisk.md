---
title: The existential risk of superintelligent AI
description: Why we all might die
published: true
---
## Experts are sounding the alarm

Over half of AI researchers think thereâ€™s a [larger than 10% risk](https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/) that a when we build a superintelligent AI, it will **destroy all life on earth**.
When you ask AI safety researchers, this number [grows to 30%](https://forum.effectivealtruism.org/posts/8CM9vZ2nnQsWJNsHx/existential-risk-from-ai-survey-results).

The [letter for pausing AI development](https://futureoflife.org/open-letter/pause-giant-ai-experiments/), launched in april 2023, has been signed over 27.000 times, mostly by AI researchers and tech leaders.

- **Stuart Russel**, writer of the #1 textbook on Artificial Intelligence
- **Geoffrey Hinton**, the "Godfather of AI" and inventor of neural network, [left Google](https://fortune.com/2023/05/01/godfather-ai-geoffrey-hinton-quit-google-regrets-lifes-work-bad-actors/) to warn people of AI.
- **Elon Musk**, co-founder of OpenAI, SpaceX and **Tesla**
- **Stephen Hawking**, theoretical physicist
- **Eliezer Yudkowski**, founder of MIRI and conceptual father of the AI safety field

## We may not have much time left

In 2020, [the average AI researcher thought](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/) that it would take until 2057 before an AI could pass SAT exams. It took us less than 3 years.

It's hard to predict how long it will take to build a superintelligent AI, but we know that there are more people than ever working on it.
The pace of progress is accelerating, and we are getting close to the point where AI might be smarter than an AI researcher.
At that point, there is a real risk of an intelligence explosion, where the AI can improve itself faster and faster, until it becomes superintelligent.

## Why superintelligence is dangerous

Intelligence can be defined as _how good something is at achieving its goals_.
Right now, humans are the most intelligent thing on earth.
We might not have claws or scaled skin, but we have big brains.
Because of our intelligence, we are dominating our planet.
We have transformed most of the earth into how we like it: cities, farms, roads.

From the perspective of less intelligent animals, this has been a disaster.
It's not that humans hate the animals, it's just that we can use their habitats for our own goals.
Our goals are things like comfort, status, love, tasty food, and more.
We are destroying the habitats of other animals as side effects of pursuing what our goals are.

An AI would have its own goals.
We know how to train machines to be intelligent, but **we don't know how to get them to want what we want**.
This problem is called the _AI alignment problem_.

If a superintelligent system is built, and it will have a goal that is even _a little_ different from what we want it to have,
it could have absolutely disastrous consequences.

Consider, for example, an AI that is given the goal to make as many paperclips as possible.


Solving this problem, before we build a superintelligent AI, is essential to our survival.
We need to pause AI development, so AI safety researchers can solve this problem, b

A superintelligent AI would be much better at achieving its goals than humans are.
It would be able to outsmart us, and get what it wants.

> It's like "Don't look up", but we are building the comet.
