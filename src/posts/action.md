---
title: Get in action
description: Ways to help out with pausing AGI development.
---

The group of people who are aware of AI risks is still small.
You are now one of them.
**Your actions matter more than ever.**

- **[Join our Discord](https://discord.gg/2XXWXvErfA)** or [other](https://coda.io/@alignmentdev/alignmentecosystemdevelopment) AI safety communities.
- **[Learn](/learn)** more about the [basics](https://www.agisafetyfundamentals.com/) of AGI x-risk and safety (the linked AGISF course is ~20-40 hours reading).
- **Talk** to people about this - friends, family, etc. Answer their questions, get them to act.
- **Share** about this on social media
- **Create** [articles](/learn#articles), [videos](/learn#videos) or [memes](https://twitter.com/AISafetyMemes)
- Organise/join **demonstrations**.
- **Lobby** politicians/industry. Talk to any relevant contacts you might have, the higher up, the better


## And

- Improve this website ([source code](https://github.com/joepio/pauseai))
- Convince journalists to write about AI safety
- Donate to the [Campaign for AI Safety](https://www.campaignforaisafety.org/donate/)
- Ask the management at your current organisation to take an institutional position on this.
- Organise and share petitions ([example](https://www.change.org/p/artificial-intelligence-time-is-running-out-for-responsible-ai-development-91f0a02c-130a-46e1-9e55-70d6b274f4df)); fund advertising for them;
- [Write to your political representatives](https://www.campaignforaisafety.org/politician/);
- Ask politicians to invite (or subpoena) AI lab leaders to parliamentary/congressional hearings to give their predictions and timelines of AI disasters;
- Make submissions to government requests for comment on AI policy ([example](https://ntia.gov/issues/artificial-intelligence/request-for-comments));
- Help draft policy ([some](https://futureoflife.org/wp-content/uploads/2023/04/FLI_Policymaking_In_The_Pause.pdf)  [frameworks](https://www.openphilanthropy.org/research/12-tentative-ideas-for-us-ai-policy/)).
- Consider [civil disobedience](https://forum.effectivealtruism.org/posts/JMb37qrCYCeKqFxtp/?commentId=xBxZEB3tx698fnsuB) / direct action.
- Consider ballot initiatives or referendums if they are achievable in your state or country
- **Donate** to advocacy orgs (there is already [Campaign for AI Safety](https://www.campaignforaisafety.org/), and more are spinning up).

If you are just starting out in AI Alignment, unless you are a genius and/or have had significant new flashes of insight on the problem, consider switching to advocacy for the Pause. Without the Pause in place first, there just isn't time to spin up a career in Alignment to the point of making useful contributions.


If you are already established in Alignment, consider more [public communication](https://twitter.com/TrustlessState/status/1651538022360285187), and adding your name to calls for the Pause and regulation of the AI industry.

## Tips for being effective

- **Be bold in your public communication of the danger**. Don't use hedging language or caveats by default; mention them when questioned, or in footnotes, but don't make it sound like you aren't that concerned if you are.
- **Be less exacting in your work**. [80/20](https://en.wikipedia.org/wiki/Pareto_principle) more. Don't do the classic EA/LW thing and spend months agonising and iterating on your Google doc over endless rounds of feedback. Get your project out into the world and iterate as you go. Time is of the essence.

But still consider downside risk: we want to act urgently but also carefully. Keep in mind that a lot of efforts to reduce AI x-risk have already backfired; alignment researchers have accidentally contributed to capabilities research, and many AI governance proposals are at danger of falling prey to industry capture.

Right now I feel like all other work is just rearranging deckchairs on the Titanic. We need to be running to the bridge, grabbing the wheel, and steering away from the iceberg. We may not have much time, but by [Good](https://en.wikipedia.org/wiki/I._J._Good) we can try. We can do this!

_Acknowledgements: Written by Greg Colbourn, [originally posted on the EA forum](https://forum.effectivealtruism.org/posts/8YXFaM9yHbhiJTPqp/agi-rising-why-we-are-in-a-new-era-of-acute-risk-and). Edited by Joep Meindertsma. For helpful comments and suggestions that have improved the post, and for the encouragement to write, I thank Akash Wasil, Johan de Kock, Jaeson Booker, Greg Kiss, Peter S. Park, Nik Samolyov, Yanni Kyriacos, Chris Leong, Alex M, Amritanshu Prasad, Dušan D. Nešić, and the rest of the [AGI Moratorium HQ Slack](https://join.slack.com/t/agi-moratorium-hq/shared_invite/zt-1u6s1opls-~_l_Ynrr~8ay~SiA2yEqAQ) and AI Notkilleveryoneism Twitter._
