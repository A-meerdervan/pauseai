---
title: Get in action
description: Ways to help out with pausing AGI development.
---

This is probably the most useful and important way to spend your time.
Thank you.

## Things anybody can do

- Join our [Discord](https://discord.gg/2XXWXvErfA) and [Slack](https://join.slack.com/t/agi-moratorium-hq/shared_invite/zt-1u6s1opls-~_l_Ynrr~8ay~SiA2yEqAQ) server and coordinate with others.
- **Spread knowledge**
  - Learn more about the [basics](https://www.agisafetyfundamentals.com/) of AGI x-risk and safety (the linked AGISF course is ~20-40 hours reading).
  - Talk to people about this - friends, family, neighbours, coworkers.
  - Be there to answer people's questions (there are a lot).
  - Post about this on Twitter and other social media (TikTok, YouTube, Instagram, Facebook etc);
  - Make and share [memes](https://twitter.com/AISafetyMemes) ([example](https://twitter.com/gcolbourn/status/1651870389985849346));
  - Write and share articles ([example](https://time.com/6273743/thinking-that-could-doom-us-with-ai/)); 
  - Fill out [this form](https://docs.google.com/forms/d/1Qrsxu5gEEAMA5imkvjk1Be7lBMgBS-zG_UNDwZfZcq0/edit) if you are interested in getting involved.
- Improve this website ([source code](https://github.com/joepio/pauseai))
- Advocate for political action: we need a Pause on AGI (possibly even a rollback) asap if we are going to get through the acute risk period. The time for talking politely to (and working with) big AI is over. It has failed. They themselves are [crying out](https://twitter.com/sama/status/1635136281952026625?lang=en-GB) to be regulated.
- Join the [AI Notkilleveryoneism Twitter Community](https://twitter.com/i/communities/1652364297345835011).
- Organise and share petitions ([example](https://www.change.org/p/artificial-intelligence-time-is-running-out-for-responsible-ai-development-91f0a02c-130a-46e1-9e55-70d6b274f4df)); fund advertising for them;
- Send letters to newspapers and magazines;
- [Write to your political representatives](https://www.campaignforaisafety.org/politician/);
- Lobby politicians/industry. Talk to any relevant contacts you might have, the higher up, the better;
- Ask politicians to invite (or subpoena) AI lab leaders to parliamentary/congressional hearings to give their predictions and timelines of AI disasters;
- Make submissions to government requests for comment on AI policy ([example](https://ntia.gov/issues/artificial-intelligence/request-for-comments));
- Help draft policy ([some](https://futureoflife.org/wp-content/uploads/2023/04/FLI_Policymaking_In_The_Pause.pdf)  [frameworks](https://www.openphilanthropy.org/research/12-tentative-ideas-for-us-ai-policy/)).
- Organise/join demonstrations.
- Consider [civil disobedience](https://forum.effectivealtruism.org/posts/JMb37qrCYCeKqFxtp/?commentId=xBxZEB3tx698fnsuB) / direct action.
- Consider ballot initiatives or referendums if they are achievable in your state or country
- Join organisations and groups working on this.
- Ask the management at your current organisation to take an institutional position on this.
- Coordinate with [other](https://twitter.com/AndrewCritchCA/status/1636203457362427908)  [groups](https://twitter.com/hashtag/TeamHuman)  [concerned](https://en.wikipedia.org/wiki/Ethics_of_artificial_intelligence) with AI who are also [pushing](https://ainowinstitute.org/wp-content/uploads/2023/04/GPAI-Policy-Brief.pdf) for regulation.
- Donate to advocacy orgs (there is already [Campaign for AI Safety](https://www.campaignforaisafety.org/), and more are spinning up).
- If you are earning or investing to give, seriously consider [joining me](https://www.facebook.com/groups/4046231355400586/posts/6444417432248621/) in liquidating a significant fraction of your assets to push this forward.

If you are just starting out in AI Alignment, unless you are a genius and/or have had significant new flashes of insight on the problem, consider switching to advocacy for the Pause. Without the Pause in place first, there just isn't time to spin up a career in Alignment to the point of making useful contributions.


If you are already established in Alignment, consider more [public communication](https://twitter.com/TrustlessState/status/1651538022360285187), and adding your name to calls for the Pause and regulation of the AI industry.

## Tips for being effective

- **Be bold in your public communication of the danger**. Don't use hedging language or caveats by default; mention them when questioned, or in footnotes, but don't make it sound like you aren't that concerned if you are.
- **Be less exacting in your work**. [80/20](https://en.wikipedia.org/wiki/Pareto_principle) more. Don't do the classic EA/LW thing and spend months agonising and iterating on your Google doc over endless rounds of feedback. Get your project out into the world and iterate as you go. Time is of the essence.

But still consider downside risk: we want to act urgently but also carefully. Keep in mind that a lot of efforts to reduce AI x-risk have already backfired; alignment researchers have accidentally contributed to capabilities research, and many AI governance proposals are at danger of falling prey to industry capture.

If you are doing other EA stuff, my feeling on this is: let's go all out to get a (strongly enforced) Pause in place, and then relax a little and go back to what we were doing before. Right now I feel like all my other work is just rearranging deckchairs on the Titanic. We need to be running to the bridge, grabbing the wheel, and steering away from the iceberg. We may not have much time, but by [Go](https://en.wikipedia.org/wiki/I._J._Good)[od](https://www.effectivealtruism.org/#:~:text=is%20about%20doing-,good,-better) we can try. C'mon EA, we can do this!

_Acknowledgements_: Written by Greg Colbourn. Edited by Joep Meindertsma. For helpful comments and suggestions that have improved the post, and for the encouragement to write, I thank Akash Wasil, Johan de Kock, Jaeson Booker, Greg Kiss, Peter S. Park, Nik Samolyov, Yanni Kyriacos, Chris Leong, Alex M, Amritanshu Prasad, Dušan D. Nešić, and the rest of the AGI Moratorium HQ Slack and AI Notkilleveryoneism Twitter. All remaining shortcomings are my own.
