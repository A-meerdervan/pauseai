---
title: PauseAI Proposal
description: We want our governments to organize a summit to pause the development of AI systems more powerful than GPT-4.
---

**Implement a temporary pause on the development of AI systems more powerful than GPT-4**.

This moratorium needs to be implemented on an international level because we cannot expect countries or companies to risk their competitive advantage by pausing AI development on their own.

An international agreement is typically established through a summit, where leaders of countries meet to discuss the issue and make a decision.

We need our leaders to understand the urgency of the situation, and to take action **right now**:

- A country needs to step up and **host a [summit](/summit)**. Pick a date and a location, then invite all UN member states.
- A **treaty** needs to be created. This treaty should specify what types of AI development are illegal, and which consequences there are to not abiding. This treaty needs to be signed by all UN member states.

## Policy

- **Ban the development and training of AI systems more powerful than GPT-4**, at least until the alignment problem is solved and the safety of such systems can be guaranteed.
- [**Track the sales of GPUs**](https://arxiv.org/abs/2303.11341) and other hardware that can be used for AI training. This helps to enforce the ban on training.
- **Ban training of AI systems on copyrighted material**. This helps with copyright issues, slows growing inequality and slows down progress towards superintelligence.
- **Set up an international AI safety agency**, similar to the IAEA. This agency will be responsible for:
  - Periodic meetings to discuss the progress of AI safety research.
  - Granting approval to conduct any new training run above a certain size (e.g. 1 billion parameters).
- **Increase national investments in AI safety research**. Right now, there exist only a few hundred AI safety researchers. This should become thousands.
- **Hold AI model creators liable** for criminal acts committed using their AI systems. This gives model creators more incentives to make sure their models are safe.
