---
title: PauseAI support gathering for Rishi Sunak @ Google DeepMind - June 8th
description: We are organizing a protest at Google DeepMind to demand a summit to pause AI development.
---

- Support gathering to encourage Rishi Sunak to take the lead on AI safety, organise a summit and implement a moratorium on AI development.
- Where: Google Deepmind, London

## Press Release

On Thursday, June 8th, volunteers from the new [PauseAI](http://pauseai.info) movement will gather outside Google DeepMind's headquarters in London to urge the UK government to take the lead on AI safety.
They are asking the Chancellor of the Exchequer, Rishi Sunak, to organize a summit to halt the development of the most capable AI systems.

A rapidly increasing number of AI experts [signed a statement](https://www.safe.ai/statement-on-ai-risk) last week that reads:

> "Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war."

This has been signed by virtually all AI labs (OpenAI, Google Deepmind, Ahthropic) and hundreds of AI scientists.

Rishi Sunak has stated that the ["Government is looking very carefully at this"](https://twitter.com/RishiSunak/status/1663838958558539776) and that ["the UK is well-placed to lead"](https://twitter.com/RishiSunak/status/1662369922234679297) the global collaboration on safe AI development.
The UK is home to some of the world's leading AI labs, including DeepMind, and has a high concentration of AI safety researchers.

The protesters are supporting Rishi Sunak in taking the lead on global AI safety.
They are asking him to organize a summit to halt the development of the most capable AI systems.
This is a different approach from what the AI lab CEOs that Rishi Sunak has spoken with have suggested.
OpenAI believes that ["it would be unintuitively risky and difficult to stop the creation of superintelligence"](https://openai.com/blog/governance-of-superintelligence), so they are pursuing further development towards superintelligence.

> "We have a choice: do we risk human extinction to build a superintelligence, or do we stop while we still can?" - PauseAI protesters

AI safety experts have not reached on consensus on how large the risk of human extinction will be.
Results from the ["Existential risk from AI survey"](https://forum.effectivealtruism.org/posts/8CM9vZ2nnQsWJNsHx/existential-risk-from-ai-survey-results) show that estimates range from 2% to 98%, with an average of 30%.

> "Even a 2% risk of human extinction is unacceptable. At the very least, pausing development deserves to be considered as a viable option, and the decision should be made democratically, not by a few AI lab CEOs." - PauseAI protesters

The protesters believe an international summit is the only way in which the world can come to a consensus on how to proceed with AI development.

> "We cannot expect nations or companies to halt development, or even to prioritize safety over capabilities, without a global agreement. Both nations and companies are incentivized to develop the most capable AI systems, and they will not stop unless a treaty is in place. One country needs to step up and take the lead in organizing this. Let it be the UK" - PauseAI protesters

For more information, please visit [PauseAI.info](http://pauseai.info).
