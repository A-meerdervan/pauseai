---
title: FAQ
---

### Who are you?

We are a group of volunteers, AI (safety) researchers and engineers who are worried about the risks of AI.
We are not affiliated with any company or organization.
You can [find us on Discord](https://discord.gg/2XXWXvErfA).

### How likely is it that superintelligent AI will cause very bad outcomes, like human extinction?

AI safety researchers (who are the experts about this topic) are divided on this question, and estimates [range from 2% to 97% with an average of 30%](https://web.archive.org/web/20221013014859/https://www.alignmentforum.org/posts/QvwSr5LsxyDeaPK5s/existential-risk-from-ai-survey-results).
Note that no (surveyed) AI safety researchers believe that there's a 0% chance.
However, there might be selection bias here: people who work in the AI safety field are likely to do so because they believe preventing bad AI outcomes is important.

If you ask AI researchers in general (not safety specialists), this number drops to a [mean value of around 14%](https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/).
Note that there might be a selection bias here as well: people who work in AI are likely to do so because they believe AI will be beneficial.

There are also some AI scientists who believe that there is no risk at all.
We have yet to see a single person who has read about AI alignment in depth, who still believes that there is no risk.

**Imagine you're invited to take a test flight on a new airplane**.
The airplane safety experts on average think there's a 30% chance it would crash.
The plane engineers think there's a 14% chance of crashing.
Some engineers are saying that there's no chance of crashing, the plane is perfectly safe.

Would you enter that plane? Because right now, we're all boarding the AI plane.

### OpenAI and Google are saying they want to be regulated. Why are you protesting them?

We applaud [OpenAI](https://openai.com/blog/governance-of-superintelligence) and [Google](https://www.ft.com/content/8be1a975-e5e0-417d-af51-78af17ef4b79) for their calls for international regulation of AI.
However, we believe that the current proposals are not enough to prevent an AI catastrophe.
Google and Microsoft have not yet publicly stated anything about the existential risk of AI.
While OpenAI [explicitly mentions the risk of extinction](https://openai.com/blog/governance-of-superintelligence), they [do not believe they have solved the alignment problem](https://youtu.be/L_Guz73e6fw?t=1478).
Both parties are locked in a race to the bottom, where AI safety is sacrificed for competitive advantage.
This is simply the result of market dynamics.
We need governments to step in and organize a summit to discuss the risks of AI, and to create a regulatory framework that will prevent the worst outcomes.

### Aren't you just scared of changes and new technology?

You might be surprised that most people in PauseAI can be considered techno-optimists.
Many of them are deeply involved in AI research and development, are gadget lovers, and have mostly been very excited about the future.
Particularly many of them have been excited about the potential of AI to help humanity.
That's why for many of us the sad realization that AI might be an existential risk was a very difficult one to internalize.

### Is a ban on AGI training enforceable?

Yes, at least for very large models.
Tracking and monitoring the sales of GPUs (used for training AIs) is [actually very doable](https://arxiv.org/abs/2304.04123).

### I have a different / AI related question

Try [AIsafety.info](https://aisafety.info/), an awesome database of questions and answers about AI safety.
